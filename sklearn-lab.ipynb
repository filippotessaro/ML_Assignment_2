{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center style=\"color: #005496; font-size: 4.2em;\">Machine Learning with Python</h1>\n",
    "<h2 align=center>Laboratory on Numpy / Matplotlib / Scikit-learn</h2>\n",
    "\n",
    "***\n",
    "\n",
    "<img src=\"https://www.dataiku.com/static/img/learn/guide/getting-started/getting-started-with-python/logo-stack-python.png\">\n",
    "\n",
    "***\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the past few years, Python has become the de-facto standard programming language for data analytics. Python's success is due to several factors, but one major reason has been the availability of powerful, open-source libraries for scientific computation such as Numpy, Scipy and Matplotlib. Python is also the most popular programming language for machine learning, thanks to libraries such as Scikit-learn and TensorFlow.\n",
    "\n",
    "In this lecture we will explore the basics of Numpy, Matplotlib and Scikit-learn. The first is a library for data manipulation through the powerfull `numpy.ndarray` data structure; the second is useful for graphical visualization and plotting; the third is a general purpose library for machine learning, containing dozens of algorithms for classification, regression and clustering.\n",
    "\n",
    "In this lecture we assume familiarity with the Python programming language. If you are not familiar with the language, we advise you to look it up before carrying over to the next sections. Here are some useful links to learn about Python:\n",
    "- https://docs.python.org/3/tutorial/introduction.html\n",
    "- https://www.learnpython.org/\n",
    "- http://www.scipy-lectures.org/\n",
    "\n",
    "If you have never seen a page like this, it is a **Jupyther Notebook**. Here one can easily embed Python code and run it on the fly. You can run the code in a cell by selecting the cell and clicking the *Run* button (top). You can do the same using the **SHIFT+Enter** shortcut. You can modify the existing cells, run them and finally save your changes.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "1. Python (preferably version > 3.3): https://www.python.org/downloads/\n",
    "2. Numpy, Scipy and Matplotlib: https://www.scipy.org/install.html\n",
    "3. Scikit-learn: http://scikit-learn.org/stable/install.html\n",
    "\n",
    "## References\n",
    "\n",
    "- https://docs.scipy.org/doc/numpy/\n",
    "- https://docs.scipy.org/doc/scipy/reference/\n",
    "- https://matplotlib.org/users/index.html\n",
    "- http://scikit-learn.org/stable/documentation.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy\n",
    "\n",
    "Numpy provides high-performance data structures for data manipulation and numeric computation. In particular, we will look at the `numpy.ndarray`, a data structure for manipulating vectors, matrices and tensors. Let's start by importing `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the np alias is very common\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can initialize a Numpy array from a Python list using the `numpy.array` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if the argument is a list of numbers, the array will be a 1-dimensional vector\n",
    "a = np.array([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if the argument is a list of lists, the array will be a 2-dimensional matrix\n",
    "M = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]])\n",
    "\n",
    "M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a Numpy array, we can check its `shape`, a tuple containing the number of elements for each dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of an array is its total number of elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "M.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do quite some nice things with Numpy arrays that are not possible with standard Python lists.\n",
    "\n",
    "### Indexing\n",
    "Numpy array allow us to index arrays in quite advanced ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A 1d vector can be indexed in all the common ways\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a[0:5:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use a boolean mask\n",
    "mask = [True, False, False, True, True, False]\n",
    "a[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Access specific elements by passing a list of index\n",
    "a[[1, 4, 5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power of Numpy indexing capabilities starts showing up with 2d arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Access a single element of the matrix\n",
    "M[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Access an entire row\n",
    "M[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Access an entire column\n",
    "M[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract a sub-matrix\n",
    "M[1:3, 0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data manipulation\n",
    "We can manipulate data in several ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Flatten a matrix\n",
    "M.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reshaping a matrix\n",
    "M.reshape(2, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The last index can be automatically inferred using -1\n",
    "M.reshape(2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Computing the max and the min\n",
    "M.max(), M.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Computing the mean and standard deviation\n",
    "M.mean(), M.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Computing the sum along the rows\n",
    "M.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear algebra\n",
    "Numpy is very useful to all sort of numeric computation, especially linear algebra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transpose\n",
    "M.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adding and multiplying a constant\n",
    "10 * M + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Element wise product\n",
    "b = np.array([-1, -2, 4, 6, 8, -4])\n",
    "\n",
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dot product\n",
    "a.dot(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# More linear algebra in the package numpy.linalg\n",
    "# Determinant\n",
    "np.linalg.det(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Eigenvalues\n",
    "np.linalg.eigvals(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector generation and sampling\n",
    "Numpy allows us to generate or randomly sample vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate an array with 0.5 spacing\n",
    "x = np.arange(-10, 10, 0.5)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate an array with 20 equally spaced points\n",
    "x = np.linspace(-10, 10, 20)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sample a vector from a standardize normal distribution\n",
    "np.random.normal(size=(10,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "Numpy provides all sorts of mathematical functions we can apply to arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exponential function\n",
    "np.exp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sine\n",
    "np.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A gaussian function\n",
    "y = np.exp(-(x ** 2)/2)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matplotlib\n",
    "The above matrices provide little insight without the possibility of visualizing them properly. Matplotlib is a powerful library for data visualization. Let's plot the above function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the following line is only needed to show plots in the notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-10, 10, 200)    # get a sample of the x axis\n",
    "y = np.exp(-(x**2)/(2*1))        # compute the function for all points in the sample\n",
    "plt.plot(x, y)                   # add the curve to the plot\n",
    "plt.ylim(-0.05,1.05)             # set bottom and top limits for y axis\n",
    "plt.show()                       # show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot more than one line in the same figure and add a grid to the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z = np.exp(-(x**2)/(2*10))\n",
    "plt.grid()                 # add the grid under the curves\n",
    "plt.plot(x, y)             # add the first curve to the plot\n",
    "plt.plot(x, z)             # add the second curve to the plot\n",
    "plt.ylim(-0.05,1.05)             # set bottom and top limits for y axis\n",
    "plt.show()                 # show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can also set several properties of the plot in this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.grid()\n",
    "plt.xlabel('x')                     # add a label to the x axis\n",
    "plt.ylabel('y')                     # add a label to the y axis\n",
    "\n",
    "plt.xticks(np.arange(-10, 11, 2))   # specify in which point to place a tick on the x axis\n",
    "plt.yticks(np.arange(0, 2.2, 0.2))  # and on the y axis\n",
    "\n",
    "# rs- stands for red, squared markers, solid line\n",
    "# yd-- stands for yellow, diamond markers, dashed line\n",
    "plt.plot(x, y, 'rs-', markevery=10, label='sigma=1')    # add a style and a label and specify the gap\n",
    "plt.plot(x, z, 'yd--', markevery=10, label='sigma=10')  # between markers for both curves\n",
    "\n",
    "plt.ylim(-0.05,1.05)             # set bottom and top limits for y axis\n",
    "plt.legend()  # add the legend (displays the labels of the curves)\n",
    "plt.show()    # show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can save the plot into a png file in this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.xticks(np.arange(-10, 11, 2))\n",
    "plt.yticks(np.arange(0, 2.2, 0.2))\n",
    "plt.plot(x, y, 'rs-', markevery=10, label='sigma=1')\n",
    "plt.plot(x, z, 'yd--', markevery=10, label='sigma=10')\n",
    "plt.ylim(-0.05,1.05)             # set bottom and top limits for y axis\n",
    "plt.legend()\n",
    "plt.savefig('plot.png', dpi=300)  # saves the plot into the file plot.png with 300 dpi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "# Scikit-learn\n",
    "Let's now dive into the real **Machine Learning** part. *Scikit-learn* is perhaps the most wide-spread library for Machine Learning in use nowadays, and most of its fame is due to its extreme simplicity. With Scikit-learn it is possible to easily manage datasets, and train a wide range of classifiers out-of-the-box. It is also useful for several other Machine Learning tasks such as regression, clustering, dimensionality reduction, and model selection.\n",
    "\n",
    "In the following we will see how to use Scikit-learn to load a dataset, train a classifier and perform validation and model selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn comes with a range of popular reference datasets. Let's load and use the *Digits* dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "print(digits.DESCR)     # print a description of the digits dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = digits.data, digits.target\n",
    "\n",
    "# The attributes of the first instance (notice it is a Numpy array)\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The label of the first instance\n",
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being a Numpy array, we can actually take a look at this image. We first need to reshape it into an 8x8 matrix and then use matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = X[0].reshape((8, 8))\n",
    "\n",
    "plt.gray()      # use a grayscale \n",
    "plt.matshow(x)  # display a matrix of values\n",
    "plt.show()      # show the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to train a classifier to recognize the digits from the images and then we want to evaluate it. In order to make a proper evaluation, we first need to split the dataset in two sets, one for training and one for testing. Scikit-learn helps us with that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "# the lab PCs have an outdated version of scikit-learn, with different names for packages \n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError:\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Let's check the length of the two sets\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a classifier. Let's use an **SVM**. A reminder:\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/2a/Svm_max_sep_hyperplane_with_margin.png/557px-Svm_max_sep_hyperplane_with_margin.png\" style=\"height: 400px\">\n",
    "\n",
    "\\begin{align}\n",
    "   \\min_{\\boldsymbol{w}} \\quad & \\frac{1}{2}\\|\\boldsymbol{w}\\|^2 + C \\sum_{i\\in|\\mathcal{D}|} \\xi_i \\\\\n",
    "   \\forall (x_i, y_i) \\in \\mathcal{D} \\quad & y_i ( \\boldsymbol{w}^T x_i + b ) \\ge 1 - \\xi_i\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Specify the parameters in the constructor.\n",
    "# C is the parameter of the primal problem of the SVM;\n",
    "# The rbf kernel is the Radial Basis Function;\n",
    "# The rbf kernel takes one parameter: gamma\n",
    "clf = SVC(C=10, kernel='rbf', gamma=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the classifier can be trained and then used to predict unseen instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prediction\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to evaluate the performance of our classifier. A reminder:\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{Accuracy } &= \\frac{\\text{true-positive} + \\text{true-negative}}{\\text{all examples}} \\\\\n",
    "    \\text{Precision } &= \\frac{\\text{true-positive}}{\\text{true-positive} + \\text{false-positive}} \\\\\n",
    "    \\text{Recall } &= \\frac{\\text{true-positive}}{\\text{true-positive} + \\text{false-negative}} \\\\\n",
    "    F_1 &= \\frac{2 \\times \\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}} \\\\\n",
    "\\end{align}\n",
    "\n",
    "In a multiclass classification Precision, Recall and $F_1$ are computed per class, considering the given class as positive and all others as negative.\n",
    "\n",
    "We can use Scikit-learn to compute and show these measures for all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "report = metrics.classification_report(y_test, y_pred)\n",
    "\n",
    "# the support is the number of instances having the given label in y_test\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can compute the accuracy of our classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently our classifier performs a bit poorly out-of-sample. This is probably due to the random choice of the parameters for the classifier. We can do much better! We need to perform model selection, that is we need to search for better parameters for our classifier.\n",
    "\n",
    "In particular, we are going to perform a **cross-validation** on the training set and see how the classifier performs with different values of *gamma*.\n",
    "\n",
    "A $k$-fold cross-validation works like this:\n",
    "- Split the dataset $D$ in $k$ equally sized disjoint subsets $D_i$\n",
    "- For $i \\in [1, k]$\n",
    " - Train the classifier on $T_i = D \\setminus D_i$\n",
    " - Compute the score (accuracy, precision, ...) on $D_i$\n",
    "- Return the list of scores, one for each fold\n",
    "\n",
    "Scikit-learn helps us with this as well. We compute the cross-validated accuracy for all the possible values of *gamma* and select the *gamma* with the best average accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, we have to add some code to have this run on the lab PCs\n",
    "try:\n",
    "    from sklearn.model_selection import KFold, cross_val_score\n",
    "    legacy = False \n",
    "except ImportError:\n",
    "    from sklearn.cross_validation import KFold, cross_val_score\n",
    "    legacy = True\n",
    "    \n",
    "    \n",
    "    \n",
    "# 3-fold cross-validation\n",
    "# random_state ensures same split for each value of gamma\n",
    "# KFold has a different syntax for legacy versions of scikit-learn\n",
    "if legacy:\n",
    "    kf = KFold(len(y_train),n_folds=3, shuffle=True, random_state=42)\n",
    "else:\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "gamma_values = [0.1, 0.05, 0.02, 0.01]\n",
    "accuracy_scores = []\n",
    "\n",
    "# Do model selection over all the possible values of gamma \n",
    "for gamma in gamma_values:\n",
    "    \n",
    "    # Train a classifier with current gamma\n",
    "    clf = SVC(C=10, kernel='rbf', gamma=gamma)\n",
    "\n",
    "    # Compute cross-validated accuracy scores\n",
    "    # So legacy....\n",
    "    if legacy: \n",
    "        scores = cross_val_score(clf, X_train, y_train, cv=kf, scoring='accuracy')\n",
    "    else:\n",
    "         scores = cross_val_score(clf, X_train, y_train, cv=kf.split(X_train), scoring='accuracy')\n",
    "    \n",
    "    # Compute the mean accuracy and keep track of it\n",
    "    accuracy_score = scores.mean()\n",
    "    accuracy_scores.append(accuracy_score)\n",
    "\n",
    "# Get the gamma with highest mean accuracy\n",
    "best_index = np.array(accuracy_scores).argmax()\n",
    "best_gamma = gamma_values[best_index]\n",
    "\n",
    "# Train over the full training set with the best gamma\n",
    "clf = SVC(C=10, kernel='rbf', gamma=best_gamma)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! Model selection allows us to fine-tune the parameters of a lerning algorithm to get the best performance. \n",
    "\n",
    "Let's now look at the **Learnig curve** of our classifier, in which we plot the training accuracy and the cross-validated accuracy for increasing number of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.model_selection import learning_curve\n",
    "except ImportError:\n",
    "    from sklearn.learning_curve import learning_curve\n",
    "    \n",
    "    \n",
    "plt.figure()\n",
    "plt.title(\"Learning curve\")\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.grid()\n",
    "\n",
    "clf = SVC(C=10, kernel='rbf', gamma=best_gamma)\n",
    "\n",
    "# Compute the scores of the learning curve\n",
    "# by default the (relative) dataset sizes are: 10%, 32.5%, 55%, 77.5%, 100% \n",
    "train_sizes, train_scores, test_scores = learning_curve(clf, X_train, y_train, scoring='accuracy', cv=3)\n",
    "\n",
    "# Get the mean and std of train and test scores along the varying dataset sizes\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot the mean and std for the training scores\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "\n",
    "# Plot the mean and std for the cross-validation scores\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "plt.ylim(0.05,1.3)             # set bottom and top limits for y axis\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to go even further. We can perform the above model selection procedure considering the *C* parameter as well. In general, this process over several parameters is called **grid search**, and Scikit-learn has an automated procedure to perform cross-validated grid search for any classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "except ImportError:\n",
    "    from sklearn.grid_search import GridSearchCV\n",
    "possible_parameters = {\n",
    "    'C': [1e0, 1e1, 1e2, 1e3],\n",
    "    'gamma': [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "}\n",
    "\n",
    "svc = SVC(kernel='rbf')\n",
    "\n",
    "# The GridSearchCV is itself a classifier\n",
    "# we fit the GridSearchCV with the training data\n",
    "# and then we use it to predict on the test set\n",
    "clf = GridSearchCV(svc, possible_parameters, n_jobs=4, cv=3) # n_jobs=4 means we parallelize the search over 4 threads\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Now we have a classifier with a quite competitive accuracy. The state-of-the-art (on a very similar task) has accuracy around $0.9979$, achieved by using Neural Networks, which we will see in the next Lab. Stay tuned!  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
